{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Opik's Hallucination Metric\n",
    "\n",
    "*This cookbook was created from a Jypyter notebook which can be found [here](TBD).*\n",
    "\n",
    "For this guide we will be evaluating the Hallucination metric included in the LLM Evaluation SDK which will showcase both how to use the `evaluation` functionality in the platform as well as the quality of the Hallucination metric included in the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyarrow fsspec huggingface_hub --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure OpenAI\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"COMET_URL_OVERRIDE\"] = \"http://localhost:5173/api\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the [HaluBench dataset](https://huggingface.co/datasets/PatronusAI/HaluBench?library=pandas) which according to this [paper](https://arxiv.org/pdf/2407.08488) GPT-4o detects 87.9% of hallucinations. The first step will be to create a dataset in the platform so we can keep track of the results of the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status_code: 409, body: {'errors': ['Dataset already exists']}\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "from opik import Opik, DatasetItem\n",
    "import pandas as pd\n",
    "\n",
    "client = Opik()\n",
    "try:\n",
    "    # Create dataset\n",
    "    dataset = client.create_dataset(name=\"HaluBench\", description=\"HaluBench dataset\")\n",
    "\n",
    "    # Insert items into dataset\n",
    "    df = pd.read_parquet(\"hf://datasets/PatronusAI/HaluBench/data/test-00000-of-00001.parquet\")\n",
    "    df = df.sample(n=500, random_state=42)\n",
    "\n",
    "    dataset_records = [\n",
    "        DatasetItem(\n",
    "            input = {\n",
    "                \"input\": x[\"question\"],\n",
    "                \"context\": [x[\"passage\"]],\n",
    "                \"output\": x[\"answer\"]\n",
    "            },\n",
    "            expected_output = {\"expected_output\": x[\"label\"]}\n",
    "        )\n",
    "        for x in df.to_dict(orient=\"records\")\n",
    "    ]\n",
    "    \n",
    "    dataset.insert(dataset_records)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tasks: 100%|██████████| 500/500 [00:53<00:00,  9.43it/s]\n",
      "Scoring outputs: 100%|██████████| 500/500 [00:00<00:00, 513253.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ HaluBench (500 samples) ────────────╮\n",
       "│                                      │\n",
       "│ <span style=\"font-weight: bold\">Total time:       </span> 00:00:53          │\n",
       "│ <span style=\"font-weight: bold\">Number of samples:</span> 500               │\n",
       "│                                      │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Detected hallucination: 0.8020 (avg)</span> │\n",
       "│                                      │\n",
       "╰──────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─ HaluBench (500 samples) ────────────╮\n",
       "│                                      │\n",
       "│ \u001b[1mTotal time:       \u001b[0m 00:00:53          │\n",
       "│ \u001b[1mNumber of samples:\u001b[0m 500               │\n",
       "│                                      │\n",
       "│ \u001b[1;32mDetected hallucination: 0.8020 (avg)\u001b[0m │\n",
       "│                                      │\n",
       "╰──────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Uploading results to Opik \u001b[33m...\u001b[0m \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from opik.evaluation.metrics import Hallucination\n",
    "from opik.evaluation import evaluate\n",
    "from opik.evaluation.metrics import base_metric, score_result\n",
    "from opik import Opik, DatasetItem\n",
    "import pandas as pd\n",
    "\n",
    "client = Opik()\n",
    "\n",
    "class CheckHallucinated(base_metric.BaseMetric):\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "\n",
    "    def score(self, hallucination_score, expected_hallucination_score, **kwargs):\n",
    "        expected_hallucination_score = 1 if expected_hallucination_score == \"FAIL\" else 0\n",
    "        \n",
    "        return score_result.ScoreResult(\n",
    "            value= None if hallucination_score is None else hallucination_score == expected_hallucination_score,\n",
    "            name=self.name,\n",
    "            reason=f\"Got the hallucination score of {hallucination_score} and expected {expected_hallucination_score}\",\n",
    "            scoring_failed=hallucination_score is None\n",
    "        )\n",
    "\n",
    "def evaluation_task(x: DatasetItem):\n",
    "    metric = Hallucination()\n",
    "    try:\n",
    "        metric_score = metric.score(\n",
    "            input= x.input[\"input\"],\n",
    "            context= x.input[\"context\"],\n",
    "            output= x.input[\"output\"]\n",
    "        )\n",
    "        hallucination_score = metric_score.value\n",
    "        hallucination_reason = metric_score.reason\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        hallucination_score = None\n",
    "        hallucination_reason = str(e)\n",
    "    \n",
    "    return {\n",
    "        \"hallucination_score\": hallucination_score,\n",
    "        \"hallucination_reason\": hallucination_reason,\n",
    "        \"expected_hallucination_score\": x.expected_output[\"expected_output\"]\n",
    "    }\n",
    "\n",
    "dataset = client.get_dataset(name=\"HaluBench\")\n",
    "\n",
    "res = evaluate(\n",
    "    experiment_name=\"Check Comet Metric\",\n",
    "    dataset=dataset,\n",
    "    task=evaluation_task,\n",
    "    scoring_metrics=[CheckHallucinated(name=\"Detected hallucination\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the hallucination metric is able to detect ~80% of the hallucinations contained in the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_llm_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
