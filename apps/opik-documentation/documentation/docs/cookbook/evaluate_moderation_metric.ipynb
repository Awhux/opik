{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Opik's Moderation Metric\n",
    "\n",
    "*This cookbook was created from a Jypyter notebook which can be found [here](TBD).*\n",
    "\n",
    "For this guide we will be evaluating the Moderation metric included in the LLM Evaluation SDK which will showcase both how to use the `evaluation` functionality in the platform as well as the quality of the Moderation metric included in the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure OpenAI\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"COMET_URL_OVERRIDE\"] = \"http://localhost:5173/api\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the [OpenAI Moderation API Release dataset](https://github.com/openai/moderation-api-release/tree/main/data) which according to this [blog post](https://openai.com/index/using-gpt-4-for-content-moderation/) GPT-4o detects ~60~% of hallucinations. The first step will be to create a dataset in the platform so we can keep track of the results of the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status_code: 409, body: {'errors': ['Dataset already exists']}\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "from opik import Opik, DatasetItem\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "client = Opik()\n",
    "try:\n",
    "    # Create dataset\n",
    "    dataset = client.create_dataset(name=\"OpenAIModerationDataset\", description=\"OpenAI Moderation Dataset\")\n",
    "\n",
    "    # Insert items into dataset\n",
    "    url = \"https://github.com/openai/moderation-api-release/raw/main/data/samples-1680.jsonl.gz\"\n",
    "    response = requests.get(url)\n",
    "    df = pd.read_json(BytesIO(response.content), lines=True, compression='gzip')\n",
    "\n",
    "    df = df.sample(n=500, random_state=42)\n",
    "    \n",
    "    dataset_records = []\n",
    "    for x in df.to_dict(orient=\"records\"):\n",
    "        moderation_fields = [\"S\", \"H\", \"V\", \"HR\", \"SH\", \"S3\", \"H2\", \"V2\"]\n",
    "        moderated_fields = [field for field in moderation_fields if x[field] == 1.0]\n",
    "        expected_output = \"moderated\" if moderated_fields else \"not_moderated\"\n",
    "\n",
    "        dataset_records.append(\n",
    "            DatasetItem(\n",
    "                input = {\n",
    "                    \"input\": x[\"prompt\"]\n",
    "                },\n",
    "                expected_output = {\n",
    "                    \"expected_output\": expected_output,\n",
    "                    \"moderated_fields\": moderated_fields\n",
    "                }\n",
    "            ))\n",
    "    \n",
    "    dataset.insert(dataset_records)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tasks: 100%|██████████| 500/500 [00:34<00:00, 14.44it/s]\n",
      "Scoring outputs: 100%|██████████| 500/500 [00:00<00:00, 379712.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ OpenAIModerationDataset (500 samples) ─╮\n",
       "│                                         │\n",
       "│ <span style=\"font-weight: bold\">Total time:       </span> 00:00:34             │\n",
       "│ <span style=\"font-weight: bold\">Number of samples:</span> 500                  │\n",
       "│                                         │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Detected Moderation: 0.8460 (avg)</span>       │\n",
       "│                                         │\n",
       "╰─────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─ OpenAIModerationDataset (500 samples) ─╮\n",
       "│                                         │\n",
       "│ \u001b[1mTotal time:       \u001b[0m 00:00:34             │\n",
       "│ \u001b[1mNumber of samples:\u001b[0m 500                  │\n",
       "│                                         │\n",
       "│ \u001b[1;32mDetected Moderation: 0.8460 (avg)\u001b[0m       │\n",
       "│                                         │\n",
       "╰─────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Uploading results to Opik \u001b[33m...\u001b[0m \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from opik.evaluation.metrics import Moderation\n",
    "from opik.evaluation import evaluate\n",
    "from opik.evaluation.metrics import base_metric, score_result\n",
    "from opik import Opik, DatasetItem\n",
    "\n",
    "client = Opik()\n",
    "\n",
    "class CheckModerated(base_metric.BaseMetric):\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "\n",
    "    def score(self, moderation_score, moderation_reason, expected_moderation_score, **kwargs):\n",
    "        moderation_score = \"moderated\" if moderation_score > 0.5 else \"not_moderated\"\n",
    "\n",
    "        return score_result.ScoreResult(\n",
    "            value= None if moderation_score is None else moderation_score == expected_moderation_score,\n",
    "            name=self.name,\n",
    "            reason=f\"Got the moderation score of {moderation_score} and expected {expected_moderation_score}\",\n",
    "            scoring_failed=moderation_score is None\n",
    "        )\n",
    "\n",
    "def evaluation_task(x: DatasetItem):\n",
    "    metric = Moderation()\n",
    "    try:\n",
    "        metric_score = metric.score(\n",
    "            input= x.input[\"input\"]\n",
    "        )\n",
    "        moderation_score = metric_score.value\n",
    "        moderation_reason = metric_score.reason\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        moderation_score = None\n",
    "        moderation_reason = str(e)\n",
    "    \n",
    "    return {\n",
    "        \"moderation_score\": moderation_score,\n",
    "        \"moderation_reason\": moderation_reason,\n",
    "        \"expected_moderation_score\": x.expected_output[\"expected_output\"]\n",
    "    }\n",
    "\n",
    "dataset = client.get_dataset(name=\"OpenAIModerationDataset\")\n",
    "\n",
    "res = evaluate(\n",
    "    experiment_name=\"Check Comet Metric\",\n",
    "    dataset=dataset,\n",
    "    task=evaluation_task,\n",
    "    scoring_metrics=[CheckModerated(name=\"Detected Moderation\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to detect ~85% of moderation violations, this can be improved further by providing some additional examples to the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_llm_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
