{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LLM Evaluation with Langchain\n",
    "\n",
    "*This cookbook was created from a Jypyter notebook which can be found [here](TBD).*\n",
    "\n",
    "For this guide, we will be performing a text to sql query generation task using LangChain. We will be using the Chinook database which contains the SQLite database of a music store with both employee, customer and invoice data.\n",
    "\n",
    "We will highlight three different parts of the workflow:\n",
    "\n",
    "1. Creating a synthetic dataset of questions\n",
    "2. Creating a LangChain chain to generate SQL queries\n",
    "3. Automating the evaluation of the SQL queries on the synthetic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our environment\n",
    "\n",
    "First, we will install the necessary libraries, download the Chinook database and set up our different API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain langchain-community langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinook database downloaded\n"
     ]
    }
   ],
   "source": [
    "# Download the relevant data\n",
    "import os\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"https://github.com/lerocha/chinook-database/raw/master/ChinookDatabase/DataSources/Chinook_Sqlite.sqlite\"\n",
    "filename = \"Chinook_Sqlite.sqlite\"\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    response = requests.get(url)\n",
    "    with open(filename, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"Chinook database downloaded\")\n",
    "    \n",
    "db = SQLDatabase.from_uri(f\"sqlite:///{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"COMET_URL_OVERRIDE\"] = \"http://localhost:5173/api\"\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a synthetic dataset\n",
    "\n",
    "In order to create our synthetic dataset, we will be using the OpenAI API to generate 20 different questions that a user might ask based on the Chinook database.\n",
    "\n",
    "In order to ensure that the OpenAI API calls are being tracked, we will be using the `track_openai` function from the `opik` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"result\": [\n",
      "    \"Which customer has made the most purchases in terms of total dollars spent?\",\n",
      "    \"What is the total number of tracks sold in each genre?\",\n",
      "    \"How many unique albums have been purchased by customers from different countries?\",\n",
      "    \"Which employee sold the most expensive track?\",\n",
      "    \"What is the average length of tracks purchased by customers from each country?\",\n",
      "    \"Which customer has spent the most money on tracks in the rock genre?\",\n",
      "    \"What is the total revenue generated by each employee?\",\n",
      "    \"How many unique artists are featured in each playlist?\",\n",
      "    \"Which customer has the highest average rating on their purchased tracks?\",\n",
      "    \"What is the total value of invoices generated by each sales support agent?\",\n",
      "    \"How many tracks have been sold to customers in each country?\",\n",
      "    \"Which artist has the most tracks featured in the top 100 selling tracks?\",\n",
      "    \"What is the total value of invoices generated in each year?\",\n",
      "    \"How many unique tracks have been purchased by customers in each city?\",\n",
      "    \"Which employee has the highest average rating on tracks they have sold?\",\n",
      "    \"What is the total number of tracks purchased by customers who have purchased tracks in the pop genre?\",\n",
      "    \"Which customer has purchased the highest number of unique tracks?\",\n",
      "    \"How many customer transactions have occurred in each year?\",\n",
      "    \"Which artist has the most tracks featured in the top 100 selling tracks in the rock genre?\",\n",
      "    \"What is the total number of tracks purchased by customers who have purchased tracks in the jazz genre?\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from opik.integrations.openai import track_openai\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "os.environ[\"COMET_PROJECT_NAME\"] = \"openai-integration\"\n",
    "client = OpenAI()\n",
    "\n",
    "openai_client = track_openai(client)\n",
    "\n",
    "prompt = \"\"\"\n",
    "Create 20 different example questions a user might ask based on the Chinook Database.\n",
    "\n",
    "These questions should be complex and require the model to think. They should include complex joins and window functions to answer.\n",
    "\n",
    "Return the response as a json object with a \"result\" key and an array of strings with the question.\n",
    "\"\"\"\n",
    "\n",
    "completion = openai_client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our synthetic dataset, we can create a dataset in Comet and insert the questions into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the synthetic dataset\n",
    "from opik import Opik\n",
    "from opik import DatasetItem\n",
    "\n",
    "synthetic_questions = json.loads(completion.choices[0].message.content)[\"result\"]\n",
    "\n",
    "client = Opik()\n",
    "try:\n",
    "    dataset = client.create_dataset(name=\"synthetic_questions\")\n",
    "    dataset.insert([\n",
    "        DatasetItem(input={\"question\": question}) for question in synthetic_questions\n",
    "    ])\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a LangChain chain\n",
    "\n",
    "We will be using the `create_sql_query_chain` function from the `langchain` library to create a SQL query to answer the question.\n",
    "\n",
    "We will be using the `CometTracer` class from the `opik` library to ensure that the LangChan trace are being tracked in Comet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT COUNT(\"EmployeeId\") AS \"TotalEmployees\" FROM \"Employee\"\n"
     ]
    }
   ],
   "source": [
    "# Use langchain to create a SQL query to answer the question\n",
    "from langchain.chains import create_sql_query_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from opik.integrations.langchain import OpikTracer\n",
    "\n",
    "os.environ[\"COMET_PROJECT_NAME\"] = \"sql_question_answering\"\n",
    "opik_tracer = OpikTracer(tags=[\"simple_chain\"])\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "chain = create_sql_query_chain(llm, db)\n",
    "response = chain.invoke({\"question\": \"How many employees are there ?\"}, {\"callbacks\": [opik_tracer]})\n",
    "response\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatting the evaluation\n",
    "\n",
    "In order to ensure our LLM application is working correctly, we will test it on our synthetic dataset.\n",
    "\n",
    "For this we will be using the `evaluate` function from the `opik` library. We will evaluate the application using a custom metric that checks if the SQL query is valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tasks: 100%|██████████| 20/20 [00:03<00:00,  5.37it/s]\n",
      "Scoring outputs: 100%|██████████| 20/20 [00:00<00:00, 82321.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ synthetic_questions (20 samples) ─╮\n",
       "│                                    │\n",
       "│ <span style=\"font-weight: bold\">Total time:       </span> 00:00:03        │\n",
       "│ <span style=\"font-weight: bold\">Number of samples:</span> 20              │\n",
       "│                                    │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">ContainsHello: 0.0000 (avg)</span>        │\n",
       "│                                    │\n",
       "╰────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─ synthetic_questions (20 samples) ─╮\n",
       "│                                    │\n",
       "│ \u001b[1mTotal time:       \u001b[0m 00:00:03        │\n",
       "│ \u001b[1mNumber of samples:\u001b[0m 20              │\n",
       "│                                    │\n",
       "│ \u001b[1;32mContainsHello: 0.0000 (avg)\u001b[0m        │\n",
       "│                                    │\n",
       "╰────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Uploading results to Opik \u001b[33m...\u001b[0m \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from opik import Opik, track\n",
    "from opik.evaluation import evaluate\n",
    "from opik.evaluation.metrics import Contains\n",
    "\n",
    "\n",
    "contains_hello = Contains(name=\"ContainsHello\")\n",
    "\n",
    "client = Opik()\n",
    "dataset = client.get_dataset(\"synthetic_questions\")\n",
    "\n",
    "@track()\n",
    "def llm_chain(input):\n",
    "    opik_tracer = OpikTracer(tags=[\"simple_chain\"])\n",
    "\n",
    "    db = SQLDatabase.from_uri(\"sqlite:///Chinook_Sqlite.sqlite\")\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "    chain = create_sql_query_chain(llm, db)\n",
    "    response = chain.invoke({\"question\": input}, {\"callbacks\": [opik_tracer]})\n",
    "    \n",
    "    return response\n",
    "\n",
    "def evaluation_task(item):\n",
    "    response = llm_chain(item.input[\"question\"])\n",
    "\n",
    "    return {\n",
    "        \"reference\": \"hello\",\n",
    "        \"output\": response\n",
    "    }\n",
    "\n",
    "res = evaluate(\n",
    "    experiment_name=\"sql_question_answering_v2\",\n",
    "    dataset=dataset,\n",
    "    task=evaluation_task,\n",
    "    scoring_metrics=[contains_hello]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_opik",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
